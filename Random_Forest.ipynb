{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxOjGEegUdw7Z2++u1zh8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanshu1303/Simple-ML-Algos-Implemented/blob/main/Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I will do the `Random Forest` algorithm."
      ],
      "metadata": {
        "id": "1-RLABeCqEa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "KiXUpg0QF-54"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_initialization(feature_index=None,threshold_value=None,left_child_node=None,right_child_node=None,class_value=None):\n",
        "  node={\n",
        "      \"feature\":feature_index,\n",
        "      \"threshold\":threshold_value,\n",
        "      \"left\":left_child_node,\n",
        "      \"right\":right_child_node,\n",
        "      \"value\":class_value\n",
        "  }\n",
        "  return node"
      ],
      "metadata": {
        "id": "tvQdJFMuHTBx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_leaf_node(node):\n",
        "  return node[\"value\"] is not None"
      ],
      "metadata": {
        "id": "DHsKEWCL77_m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples_split=2\n",
        "max_depth=10\n",
        "n_features=None\n",
        "root=None"
      ],
      "metadata": {
        "id": "Q3OyweRS-J4q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_decision_tree(X,y,_min_samples_split=2,_max_depth=100,_n_features=None):\n",
        "  global min_samples_split,max_depth,n_features,root\n",
        "  min_samples_split=_min_samples_split\n",
        "  max_depth=_max_depth\n",
        "  n_features=_n_features\n",
        "  root=grow_tree(X,y,0)\n",
        "  return root"
      ],
      "metadata": {
        "id": "QgL-ZGwU-6ip"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grow_tree(X,y,depth=0,min_samples_split=2,max_depth=100,n_features=None):\n",
        "  n_samples, n_feats=X.shape\n",
        "\n",
        "  n_labels=len(np.unique(y))\n",
        "\n",
        "  # Check if we're already at the stopping limit using 3 conditions\n",
        "  # * if the current depth is already equal to the max depth allowed\n",
        "  # * if all the samples are of a single class(i.e. the samples are 100% pure)\n",
        "  # * the number of samples are less than the minimum number of samples that we decided to stop\n",
        "  # splitting on\n",
        "  if (depth >= max_depth) or (n_labels == 1) or (n_samples <= min_samples_split):\n",
        "    leaf_value=most_common_label(y)\n",
        "    return create_node(value=leaf_value)\n",
        "\n",
        "  feat_idxs = np.random.choice(n_feats,n_features if n_features else n_feats,replace=False)\n",
        "  # Above, we choose what features to train our tree on. If we have been provided with 'n_features'\n",
        "  # as an arguement to the function, we obviously use that but if we haven't then we use the n_feats\n",
        "  # value\n",
        "\n",
        "  # Now, we focus on finding the best split among all the splits possible\n",
        "  # Also, for a particular feature, since it may not always be a binary one, we also have to\n",
        "  # decide what threshold we choose i.e. where we draw a line for a particular feature's values\n",
        "  best_feature, best_thresh = best_split(X,y,feat_idxs,min_samples_split)\n",
        "\n",
        "  # Now that we know what the best feature to split is, we actually split the nodes\n",
        "  left_idxs, right_idxs = split(X[:,best_feature],best_thresh)\n",
        "  left=grow_tree(X[left_idxs,:],y[left_idxs],depth+1,min_samples_split,max_depth,n_features)\n",
        "  right=grow_tree(X[right_idxs,:],y[right_idxs],depth+1,min_samples_split,max_depth,n_features)\n",
        "\n",
        "  return create_node(feature=best_feature,threshold=best_thresh,left=left,right=right)"
      ],
      "metadata": {
        "id": "X3HINqUVA6no"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_node(feature=None, threshold=None, left=None, right=None, value=None):\n",
        "  return {\"feature\":feature, \"threshold\":threshold, \"left\":left, \"right\":right, \"value\":value}"
      ],
      "metadata": {
        "id": "kjP3L7apUc1Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(X,y,feat_idxs,min_samples_split):\n",
        "  best_gain=-1\n",
        "  split_idx=None\n",
        "  split_threshold=None\n",
        "\n",
        "  for feat_idx in feat_idxs:\n",
        "    X_column=X[:,feat_idx]\n",
        "    thresholds=np.unique(X_column)\n",
        "    # ie the 'threshold' we choose wont be derived by some mathematical calculation but only\n",
        "    # from the values already existing in the set\n",
        "    for threshold in thresholds:\n",
        "      #gain=information_gain(y,X_column, threshold, min_samples_split)\n",
        "      gain=information_gain(y,X_column, threshold)\n",
        "\n",
        "\n",
        "      if gain>best_gain:\n",
        "        best_gain=gain\n",
        "        split_idx=feat_idx\n",
        "        split_threshold=threshold\n",
        "\n",
        "  return split_idx, split_threshold"
      ],
      "metadata": {
        "id": "X_l2DQ1oWVMN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(y,X_column, threshold):\n",
        "  # Calculate the entropy of the parent\n",
        "  parent_entropy=entropy(y)\n",
        "\n",
        "  # Now, create children using the threshold value\n",
        "  left_idxs, right_idxs=split(X_column, threshold)\n",
        "\n",
        "  if((len(left_idxs)==0) or (len(right_idxs)==0)):\n",
        "    return 0\n",
        "\n",
        "  n=len(y)\n",
        "\n",
        "  n_l=len(left_idxs)\n",
        "  e_l=entropy(y[left_idxs])\n",
        "\n",
        "  n_r=len(right_idxs)\n",
        "  e_r=entropy(y[right_idxs])\n",
        "\n",
        "  child_entropy=(n_l/n)*(e_l)+(n_r/n)*(e_r)\n",
        "\n",
        "  return (parent_entropy - child_entropy)"
      ],
      "metadata": {
        "id": "SOWleBZ2YlAK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(X_column, split_thresh):\n",
        "  left_idxs = np.argwhere(X_column<=split_thresh).flatten()\n",
        "  right_idxs= np.argwhere(X_column >split_thresh).flatten()\n",
        "  return left_idxs, right_idxs"
      ],
      "metadata": {
        "id": "Wb6BkmaX88oG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "  hist=np.bincount(y)\n",
        "  ps=hist/len(y)\n",
        "  # The above provides us with the occurence ratio for each index where we interpret the indices\n",
        "  # as class\n",
        "  return -np.sum([p*np.log2(p) for p in ps if p>0])"
      ],
      "metadata": {
        "id": "csmwdv2MAyRS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_label(y):\n",
        "  counter=Counter(y)\n",
        "  most_common=counter.most_common(1)[0][0]\n",
        "  return most_common"
      ],
      "metadata": {
        "id": "cYahXo-5FLIt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X,root):\n",
        "  return np.array([traverse_tree(x,root) for x in X])"
      ],
      "metadata": {
        "id": "iMjZiutrN2HQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traverse_tree(x,node):\n",
        "  if is_leaf_node(node):\n",
        "    return node[\"value\"]\n",
        "\n",
        "  if x[node[\"feature\"]]<=node[\"threshold\"]:\n",
        "    return traverse_tree(x,node[\"left\"])\n",
        "\n",
        "  else:\n",
        "    return traverse_tree(x,node[\"right\"])"
      ],
      "metadata": {
        "id": "cGK44jgaOaR9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_test, y_pred):\n",
        "    return np.sum(y_test == y_pred) / len(y_test)"
      ],
      "metadata": {
        "id": "XM3tnAUAP2GL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "\n",
        "# def fit_decision_tree(X,y,_min_samples_split=2,_max_depth=100,_n_features=None)\n",
        "fit_decision_tree(X_train, y_train, _min_samples_split=2, _max_depth=10, _n_features=None)\n",
        "\n",
        "\n",
        "predictions = predict(X_test, root)\n",
        "\n",
        "print(f\"Accuracy: {100*accuracy(y_test, predictions)} percent\")"
      ],
      "metadata": {
        "id": "ODrgmsLIQgX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb13557-5db7-41f0-bfc1-8b04783b2184"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.10526315789474 percent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code is the same code that I used in the decision tree model."
      ],
      "metadata": {
        "id": "wMii53RTj9Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is because as the name suggests, a `Random Forest` is a forest i.e. in this context, a collection of trees. It is called random because the set of examples that we use to train a single tree in the forest is decided randomly."
      ],
      "metadata": {
        "id": "gaXlVTPSkTZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How this works is that before using this for predictions, we train as many trees as we want to and then for the predictions:\n",
        "* if it is a classification problem, we choose the class that occurs most frequently\n",
        "* if it is a regression problem, we average out the predictions\n",
        "\n",
        "to arrive at our solution."
      ],
      "metadata": {
        "id": "kS4mEBO6l6eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we begin actually coding up our random forest model building upon the decision tree code previously written."
      ],
      "metadata": {
        "id": "CJTQ2lMAqbjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_trees=10\n",
        "max_depth=10\n",
        "min_samples_split=2\n",
        "n_features=None\n",
        "trees=[]"
      ],
      "metadata": {
        "id": "Ceb6jnRuqmnJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just declared some global variables so that we can use them everywhere in all functions without having to pass them as arguments to every function where we want to use them.\n",
        "\n",
        "Now, we write a simple function to initialize our forest."
      ],
      "metadata": {
        "id": "B5xxix3pqvtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_random_forest(_n_trees=10,_max_depth=10,_min_samples_split=2,_n_features=None):\n",
        "  global n_trees,max_depth,min_samples_split,n_features,trees\n",
        "  n_trees=_n_trees\n",
        "  max_depth=_max_depth\n",
        "  min_samples_split=_min_samples_split\n",
        "  n_features=_n_features\n",
        "  trees=[] # Do this to reset the list of trees and eliminate trees that may have been built\n",
        "           # previously"
      ],
      "metadata": {
        "id": "ux7CkpEtthfN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we write the `build_random_forest` function that will present us with the trained model."
      ],
      "metadata": {
        "id": "O78vTAMku1yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_random_forest(X,y,_n_trees=10,_max_depth=10,_min_samples_split=2,_n_features=None):\n",
        "  global trees\n",
        "\n",
        "  for _ in range(_n_trees):\n",
        "    X_sample, y_sample=bootstrap_samples(X,y)\n",
        "    # The code wouldnt work because the original 'fit_decision_tree' function didnt return anything\n",
        "    # It just updated the global variables that were there when we were using it for the decision\n",
        "    # trees. I updated it to return the root of the tree, which is now\n",
        "    # how we represent a tree\n",
        "    # This is crucial because the decison tree's predict function also takes the tree's root\n",
        "    # to traverse it. Hence, it makes sense to represent and pass around a tree by its root node.\n",
        "    tree=fit_decision_tree(X_sample,y_sample,_min_samples_split=_min_samples_split,\n",
        "                           _max_depth=_max_depth,_n_features=_n_features)\n",
        "    trees.append(tree)"
      ],
      "metadata": {
        "id": "djvk8aAWvaD1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent our code from getting too bulky, we write a sample `bootstrap_samples` function that creates samples to train our decision tree on."
      ],
      "metadata": {
        "id": "qTAb1pE_wsYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creates the dataset for a particular tree by choosing data points from the dataset randomly but **with replacement**.\n",
        "\n",
        "What this means is that when we choose a sample, after 'choosing' it, we put it back in our dataset. This allows it for the possibility for the same sample to be chosen again.\n",
        "\n",
        "Hence, when we use this method to build a dataset, some samples from our original dataset might occur more than once in our bootstrapped dataset while some samples might not occur at all.\n",
        "\n",
        "This randomness in the datasets allows for very different decision trees to be built with significantly different `node splits` at each level which in turn allow for a far more robust final decision tree model which doesn't react very strongly to minor changes in the dataset and is largely immune to small alterations in the data."
      ],
      "metadata": {
        "id": "Yjkj8y3Mxtqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_samples(X,y):\n",
        "  n_samples=X.shape[0]\n",
        "  idxs=np.random.choice(n_samples,n_samples,replace=True)\n",
        "  return X[idxs],y[idxs]"
      ],
      "metadata": {
        "id": "KmrPTFLkz2Tt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we only need to write the `predict` function"
      ],
      "metadata": {
        "id": "P1xZl7X80Mu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_random_forest(X):\n",
        "  predictions=np.array([predict(X,tree) for tree in trees])\n",
        "  predictions=np.swapaxes(predictions,0,1)\n",
        "  #predictions=predictions.T\n",
        "  final_predictions=np.array([most_common_label(pred) for pred in predictions])\n",
        "\n",
        "  return final_predictions"
      ],
      "metadata": {
        "id": "LwenFe-w1fkN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code, while looks simple, actually does a lot.\n",
        "\n",
        "First, obviously, for the test dataset or the dataset we're given to predict, we get the prediction for all samples from all the trees in it and store it in a numpy array.\n",
        "\n",
        "Now, the `predictions` array actually is a bit inconvenient for us. In it, each row contains the predictions from a particular tree for all the rows in X.\n",
        "\n",
        "i.e. the 1st row would be the model's 1st tree's predictions for all the rows in X, and so on. So now, the 1st column contains all the trees' predictions for the 1st row in X.\n",
        "\n",
        "But, we need to predict a particular row's class using all the forests' trees predictions. For this, we would like for all the trees' predictions for a particular row to be in the same row.\n",
        "\n",
        "This can be achieved easily with a simple transpose of the matrix i.e. here, a swap of the axes of the array.\n",
        "\n",
        "Now, the 1st row has all of the models' trees predictions for it.\n",
        "\n",
        "Then, going row by row, we simply choose the most commonly occuring label and make it the predicted class for that row."
      ],
      "metadata": {
        "id": "uQTv14sc2hHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "\n",
        "# Initialzing the random forest with all its parameters and its values\n",
        "initialize_random_forest(_n_trees=10, _max_depth=10, _min_samples_split=2, _n_features=None)\n",
        "\n",
        "# Here we build the random forest model using the training data\n",
        "build_random_forest(X_train, y_train, _n_trees=100, _max_depth=10, _min_samples_split=2, _n_features=None)\n",
        "\n",
        "# Making predictions on the test set using the random forest model\n",
        "predictions_rf = predict_random_forest(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "print(f\"Random Forest Accuracy: {100*accuracy(y_test, predictions_rf)} percent\")\n",
        "\n",
        "# Additionally, we calculate and print the F1 score\n",
        "f1 = f1_score(y_test, predictions_rf, average='binary')  # Adjust 'average' as necessary\n",
        "print(f\"Random Forest F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-n1iOviI6Yu",
        "outputId": "f649bc24-1c93-4c25-cc36-98ddda93d15a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 93.85964912280701 percent\n",
            "Random Forest F1 Score: 0.9496402877697843\n"
          ]
        }
      ]
    }
  ]
}