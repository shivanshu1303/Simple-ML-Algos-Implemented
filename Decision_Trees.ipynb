{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmFDumDxM4Ujz7FSViKOTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanshu1303/Simple-ML-Algos-Implemented/blob/main/Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here, I will implement decision trees. They are useful algorithms that help a lot in classification problems.\n",
        "## Like what the notion of a `tree` would suggest, these have nodes too that are formed after splitting the dataset using some particular feature into child nodes that either have/ don't have the feature."
      ],
      "metadata": {
        "id": "POJn3Pthi3BF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One of the fundamental concepts here is that of `entropy`, a function whose calculation is similar to that of the cost function for logistic regression(another algorithm primarily used for classification problem)"
      ],
      "metadata": {
        "id": "OL7axs_FksbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "1Kt8lLAVlrZd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "  if(y==0 or y==1):\n",
        "    return 0\n",
        "  else:\n",
        "    return -((y*np.log2(y))+((1-y)*(np.log2(1-y))))"
      ],
      "metadata": {
        "id": "XB-z-cnalpWt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After this, we have to actually split the indices i.e. form 2 child nodes from 1 parent node"
      ],
      "metadata": {
        "id": "F_b6IIjHuhWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If feature=1 - > left node\n",
        "otherwise    - > right node\n",
        "\"\"\"\n",
        "def split_node(X,feature_number):\n",
        "  left_indices=[]\n",
        "  right_indices=[]\n",
        "\n",
        "  for i,x in enumerate(X):\n",
        "    if x[feature_number] ==1:\n",
        "      left_indices.append(i)\n",
        "    else:\n",
        "      right_indices.append(i)\n",
        "\n",
        "  return left_indices, right_indices"
      ],
      "metadata": {
        "id": "bOHbIO-qvNJ_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The next thing we do is calculate the `weighted entropy`.\n",
        "### To do this(in our case where we split the node), for each of the child nodes, we multiply 2 values:\n",
        "* The ratio of: the number of samples in the child divided by the number of samples in the parent node\n",
        "* For the child node, the value of the entropy calculated for the value obtained after dividing the number of positive samples (`y[index]=1`) to the number of total samples"
      ],
      "metadata": {
        "id": "LogyjJSTozan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_entropy(X,y,left_indices,right_indices):\n",
        "  w_left=len(left_indices)/len(X)\n",
        "  w_right=len(right_indices)/len(X)\n",
        "  p_left=sum(y[left_indices])/len(left_indices)\n",
        "  p_right=sum(y[right_indices])/len(right_indices)\n",
        "\n",
        "  weighted_entropy=(w_left)*(entropy(p_left))+(w_right)*(entropy(p_right))\n",
        "\n",
        "  return weighted_entropy"
      ],
      "metadata": {
        "id": "IYQaA1Z-6YFw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This split at any node can be performed for various features i.e. in multiple manners.\n",
        "## How do we choose which is the best?\n",
        "## We calculate the 'information gain' that occurs in case of each possible split\n",
        "## This is calculated by\n",
        "* Calculating the information in the `parent node`(using the weighted entropy method) as - weight would be 1 since it has all the samples there are and then we calculate the entropy @ the value we get by dividing the no. of positive samples by no. of total samples in the node\n",
        "* Calculating the `weighted entropy` by the above method for the child nodes obtained after any particular split\n",
        "\n",
        "### Then, we subtract the above 2 values"
      ],
      "metadata": {
        "id": "DUbqn94o8r8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def info_gain(X,y,left_indices,right_indices):\n",
        "  info_parent=(1)*entropy((sum(y))/(len(y)))\n",
        "\n",
        "  info_children=weighted_entropy(X,y,left_indices,right_indices)\n",
        "\n",
        "  info_gain=info_parent-info_children\n",
        "\n",
        "  return info_gain"
      ],
      "metadata": {
        "id": "-8cwHCKF_hgp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[1, 1, 1],\n",
        "[0, 0, 1],\n",
        " [0, 1, 0],\n",
        " [1, 0, 1],\n",
        " [1, 1, 1],\n",
        " [1, 1, 0],\n",
        " [0, 0, 0],\n",
        " [1, 1, 0],\n",
        " [0, 1, 0],\n",
        " [0, 1, 0]])\n",
        "\n",
        "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])\n",
        "\n",
        "left_indices,right_indices = split_node(X_train, 0)\n",
        "\n",
        "info_gain(X_train, y_train, left_indices, right_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUMRCKwnAWsM",
        "outputId": "20f9848d-149c-4558-c7c2-d671bcbfcb4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2780719051126377"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naturally, for all different split possible and all consequent child nodes 'birthed', they'll have different values of the information gain.\n",
        "\n",
        "## The best split is the one with the **highest information gain value**"
      ],
      "metadata": {
        "id": "U_rWpXl2Aw_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features=([\"Feature 1\",\"Feature 2\",\"Feature 3\"])\n",
        "for i,feature_name in enumerate(features):\n",
        "  left_indices,right_indices=split_node(X_train,i)\n",
        "  information_gain=info_gain(X_train,y_train,left_indices,right_indices)\n",
        "\n",
        "  print(f\"The info gain when split by feature number {i} i.e {features[i]} is:{information_gain:0.04f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLwG58UIBDoY",
        "outputId": "7f84f13a-8643-47e2-e990-2d08a68ce4fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The info gain when split by feature number 0 i.e Feature 1 is:0.2781\n",
            "The info gain when split by feature number 1 i.e Feature 2 is:0.0349\n",
            "The info gain when split by feature number 2 i.e Feature 3 is:0.1245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the above code block, we see that the 3 features that we could split a particular node on have differing values of the resulting information gain.\n",
        "\n",
        "## The best split is the one with the highest information gain i.e. here, Feature 1"
      ],
      "metadata": {
        "id": "bpuZZ5PiV6ab"
      }
    }
  ]
}