{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH8LuCsYIVc3R5z59AZY6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanshu1303/Simple-ML-Algos-Implemented/blob/main/Decision_Trees_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here is another implementation of `Decision Trees`. The first one is coded for when the features take on binary(i.e. 0/1) values.\n",
        "## Binary values however don't happen very often and most datasets we will deal with will require us to deal with features take on a wider or even continuous range of values.\n",
        "This tree implementation would be more suited to that task."
      ],
      "metadata": {
        "id": "APjuabMsFbUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We of course start with importing the requred libraries"
      ],
      "metadata": {
        "id": "4hGdtE37F5fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "KiXUpg0QF-54"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, our implementation will actually use 2 different kinds of data structures. The 1st would be the tree itself but the 2nd would be one we would be one we use to represent the nodes in our tree.\n",
        "\n",
        "The functions we use will also reflect that in their nomenclature i.e. the way they are named."
      ],
      "metadata": {
        "id": "7Mpxfz4uGAe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we do the node functions since there are only a couple of them and thus they are quite easy to complete(& to understand)."
      ],
      "metadata": {
        "id": "raY8C7b8HKV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_initialization(feature_index=None,threshold_value=None,left_child_node=None,right_child_node=None,class_value=None):\n",
        "  node={\n",
        "      \"feature\":feature_index,\n",
        "      \"threshold\":threshold_value,\n",
        "      \"left\":left_child_node,\n",
        "      \"right\":right_child_node,\n",
        "      \"value\":class_value\n",
        "  }\n",
        "  return node"
      ],
      "metadata": {
        "id": "tvQdJFMuHTBx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see above, we store each node as a dictionary. We also have to differentiate a leaf node i.e. a terminal node from other nodes. We obviously write a function to do that."
      ],
      "metadata": {
        "id": "O3kzXCvN7gWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_leaf_node(node):\n",
        "  return node[\"value\"] is not None"
      ],
      "metadata": {
        "id": "DHsKEWCL77_m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're done with functions relating to nodes. We focus on the tree."
      ],
      "metadata": {
        "id": "mvDcARez8ewv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we just write down some global variables that we would use throughout our tree code so that we dont have to pass them over and over again.\n",
        "\n",
        "These would be the 'stopping criteria' for our tree (i.e. the number of samples below which we will stop splitting & the maximum depth we will allow our tree to reach) and a few others that are mostly syntactic and implementation-related nitty-gritty."
      ],
      "metadata": {
        "id": "f0LO6fp99F9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_samples_split=2\n",
        "max_depth=10\n",
        "n_features=None\n",
        "root=None"
      ],
      "metadata": {
        "id": "Q3OyweRS-J4q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, just like we would with a decision tree, we write the `fit` function to fit the tree to."
      ],
      "metadata": {
        "id": "_Ziu44Ia-Szb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_decision_tree(X,y,_min_samples_split=2,_max_depth=100,_n_features=None):\n",
        "  global min_samples_split,max_depth,n_features,root\n",
        "  min_samples_split=_min_samples_split\n",
        "  max_depth=_max_depth\n",
        "  n_features=_n_features\n",
        "  root=grow_tree(X,y,0)"
      ],
      "metadata": {
        "id": "QgL-ZGwU-6ip"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, you might have seen a new word - `global`. This only means that we want to use some variables that are 'global' variables and dont want to create new variables (that would be destroyed as soon as we exit the function)."
      ],
      "metadata": {
        "id": "8hXtdZG7AYUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, we do fairly standard assignment operations. Finally, we call the `grow_tree` fucntion, which as the name suggests is used to **grow** our tree."
      ],
      "metadata": {
        "id": "YPGFXlo-AqAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grow_tree(X,y,depth=0,min_samples_split=2,max_depth=100,n_features=None):\n",
        "  n_samples, n_feats=X.shape\n",
        "\n",
        "  n_labels=len(np.unique(y))\n",
        "\n",
        "  # Check if we're already at the stopping limit using 3 conditions\n",
        "  # * if the current depth is already equal to the max depth allowed\n",
        "  # * if all the samples are of a single class(i.e. the samples are 100% pure)\n",
        "  # * the number of samples are less than the minimum number of samples that we decided to stop\n",
        "  # splitting on\n",
        "  if (depth >= max_depth) or (n_labels == 1) or (n_samples <= min_samples_split):\n",
        "    leaf_value=most_common_label(y)\n",
        "    return create_node(value=leaf_value)\n",
        "\n",
        "  feat_idxs = np.random.choice(n_feats,n_features if n_features else n_feats,replace=False)\n",
        "  # Above, we choose what features to train our tree on. If we have been provided with 'n_features'\n",
        "  # as an arguement to the function, we obviously use that but if we haven't then we use the n_feats\n",
        "  # value\n",
        "\n",
        "  # Now, we focus on finding the best split among all the splits possible\n",
        "  # Also, for a particular feature, since it may not always be a binary one, we also have to\n",
        "  # decide what threshold we choose i.e. where we draw a line for a particular feature's values\n",
        "  best_feature, best_thresh = best_split(X,y,feat_idxs,min_samples_split)\n",
        "\n",
        "  # Now that we know what the best feature to split is, we actually split the nodes\n",
        "  left_idxs, right_idxs = split(X[:,best_feature],best_thresh)\n",
        "  left=grow_tree(X[left_idxs,:],y[left_idxs],depth+1,min_samples_split,max_depth,n_features)\n",
        "  right=grow_tree(X[right_idxs,:],y[right_idxs],depth+1,min_samples_split,max_depth,n_features)\n",
        "\n",
        "  return create_node(feature=best_feature,threshold=best_thresh,left=left,right=right)"
      ],
      "metadata": {
        "id": "X3HINqUVA6no"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now code the `create_node` function that would generate a node using the values that we passed to it."
      ],
      "metadata": {
        "id": "R9c_AsFITp3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_node(feature=None, threshold=None, left=None, right=None, value=None):\n",
        "  return {\"feature\":feature, \"threshold\":threshold, \"left\":left, \"right\":right, \"value\":value}"
      ],
      "metadata": {
        "id": "kjP3L7apUc1Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this, we code the `best_split` function we used in the `grow_tree` function that would provide us with the ideal feature to split on and the ideal threshold to divide samples"
      ],
      "metadata": {
        "id": "VcTTzlM3WIYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(X,y,feat_idxs,min_samples_split):\n",
        "  best_gain=-1\n",
        "  split_idx=None\n",
        "  split_threshold=None\n",
        "\n",
        "  for feat_idx in feat_idxs:\n",
        "    X_column=X[:,feat_idx]\n",
        "    thresholds=np.unique(X_column)\n",
        "    # ie the 'threshold' we choose wont be derived by some mathematical calculation but only\n",
        "    # from the values already existing in the set\n",
        "    for threshold in thresholds:\n",
        "      #gain=information_gain(y,X_column, threshold, min_samples_split)\n",
        "      gain=information_gain(y,X_column, threshold)\n",
        "\n",
        "\n",
        "      if gain>best_gain:\n",
        "        best_gain=gain\n",
        "        split_idx=feat_idx\n",
        "        split_threshold=threshold\n",
        "\n",
        "  return split_idx, split_threshold"
      ],
      "metadata": {
        "id": "X_l2DQ1oWVMN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was a very simple implementation if we look at it. We just go feature by feature and for each feature, we try all possible values of the thresholds and just choose the best one. Kind of like a nested loop, if you will."
      ],
      "metadata": {
        "id": "5HR4Z7s6XlHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we move on to calculate the `information_gain` function."
      ],
      "metadata": {
        "id": "XfRHg-B8YJ2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(y,X_column, threshold):\n",
        "  # Calculate the entropy of the parent\n",
        "  parent_entropy=entropy(y)\n",
        "\n",
        "  # Now, create children using the threshold value\n",
        "  left_idxs, right_idxs=split(X_column, threshold)\n",
        "\n",
        "  if((len(left_idxs)==0) or (len(right_idxs)==0)):\n",
        "    return 0\n",
        "\n",
        "  n=len(y)\n",
        "\n",
        "  n_l=len(left_idxs)\n",
        "  e_l=entropy(y[left_idxs])\n",
        "\n",
        "  n_r=len(right_idxs)\n",
        "  e_r=entropy(y[right_idxs])\n",
        "\n",
        "  child_entropy=(n_l/n)*(e_l)+(n_r/n)*(e_r)\n",
        "\n",
        "  return (parent_entropy - child_entropy)"
      ],
      "metadata": {
        "id": "SOWleBZ2YlAK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're mostly done with the functionality."
      ],
      "metadata": {
        "id": "iEXuaRYT8qkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only the 'helper functions' remain. They help make the code look more readable by taking out the details so that we can focus better on the algorithm and not the syntactic nitty-gritty."
      ],
      "metadata": {
        "id": "asKdli9N8ueG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first one among which is the `split` function that divides the node samples into 2 parts using their indices."
      ],
      "metadata": {
        "id": "sVmDhX1b9Uru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split(X_column, split_thresh):\n",
        "  left_idxs = np.argwhere(X_column<=split_thresh).flatten()\n",
        "  right_idxs= np.argwhere(X_column >split_thresh).flatten()\n",
        "  return left_idxs, right_idxs"
      ],
      "metadata": {
        "id": "Wb6BkmaX88oG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we go on to the `entropy` function"
      ],
      "metadata": {
        "id": "83T_WyUuAuEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "  hist=np.bincount(y)\n",
        "  ps=hist/len(y)\n",
        "  # The above provides us with the occurence ratio for each index where we interpret the indices\n",
        "  # as class\n",
        "  return -np.sum([p*np.log2(p) for p in ps if p>0])"
      ],
      "metadata": {
        "id": "csmwdv2MAyRS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the really smart thing is the use of the `bincount` function. What this does is it returns the frequency of ocurence of each element in the array.\n",
        "\n",
        "Please see the code-block below:\n",
        "\n",
        "`np.bincount(np.array([1,2,3,1,2]))`\n",
        "\n",
        "`>> array([0, 2, 2, 1])`"
      ],
      "metadata": {
        "id": "rQIpTYXcDyPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be interpreted as: `0` occurs zero times in the array, `1` and `2` occur two times in the array and `3` occurs 1 time in the array.\n",
        "\n",
        "It returns the frequency of the index in the array. This works beautifully for us in helping us calculate probabilities."
      ],
      "metadata": {
        "id": "WVEHmnWuETOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function is the `most_common_label` function that would return use the most common label for a sample\n",
        "\n",
        "i.e. per our model, the class of the label."
      ],
      "metadata": {
        "id": "zJN1pyy-Ep3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_label(y):\n",
        "  counter=Counter(y)\n",
        "  most_common=counter.most_common(1)[0][0]\n",
        "  return most_common"
      ],
      "metadata": {
        "id": "cYahXo-5FLIt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The interesting part in the above function might be the amount of numbers we have to use in the right hand side of the line calculating `most_common`.\n",
        "\n",
        "The 1st number i.e. `1` is the argument to the `most_common` method which tells it to return the 1 most common value in `y` i.e. the most frequently occuring value.\n",
        "\n",
        "What it returns is a list. Each element of the list is a tuple.\n",
        "\n",
        "The tuple has 2 elements:\n",
        "\n",
        "* The first is the value in the argument passed to the counter.\n",
        "* The second is the frequency of occurence of the value.\n",
        "\n",
        "So, when we write `.most_common(1)[0][0]`, it returns the most common element as a tuple inside a list.\n",
        "\n",
        "The 1st zero tells the program to access the first tuple and the 2nd zero tells it to access the 1st element in the tuple i.e. the most frequently occuring value."
      ],
      "metadata": {
        "id": "StlNjoJvFtaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we also do the `predict` helper function. This will provide us with results after we have trained our model."
      ],
      "metadata": {
        "id": "EuMP1XQ8MZ3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X,root):\n",
        "  return np.array([traverse_tree(x,root) for x in X])"
      ],
      "metadata": {
        "id": "iMjZiutrN2HQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we do the `traverse_tree` function that would go down the tree and fetch a result value."
      ],
      "metadata": {
        "id": "YTKokbPEOT3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def traverse_tree(x,node):\n",
        "  if is_leaf_node(node):\n",
        "    return node[\"value\"]\n",
        "\n",
        "  if x[node[\"feature\"]]<=node[\"threshold\"]:\n",
        "    return traverse_tree(x,node[\"left\"])\n",
        "\n",
        "  else:\n",
        "    return traverse_tree(x,node[\"right\"])"
      ],
      "metadata": {
        "id": "cGK44jgaOaR9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're done with everything the model might need.\n",
        "\n",
        "We only have to add the accessory `accuracy` function so that we know our model's performance."
      ],
      "metadata": {
        "id": "gZGg6pCJPp6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_test, y_pred):\n",
        "    return np.sum(y_test == y_pred) / len(y_test)"
      ],
      "metadata": {
        "id": "XM3tnAUAP2GL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We are done with the model"
      ],
      "metadata": {
        "id": "0RQll_a8P6OM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have to train it on data to see if it works well or not."
      ],
      "metadata": {
        "id": "wWUjhh-LQcS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "\n",
        "# def fit_decision_tree(X,y,_min_samples_split=2,_max_depth=100,_n_features=None)\n",
        "fit_decision_tree(X_train, y_train, _min_samples_split=2, _max_depth=100, _n_features=None)\n",
        "\n",
        "\n",
        "predictions = predict(X_test, root)\n",
        "\n",
        "print(f\"Accuracy: {100*accuracy(y_test, predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODrgmsLIQgX4",
        "outputId": "e76a83a4-d8e8-4395-8d43-f679e44ba93d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 91.22807017543859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see above, on the `breast-cancer` dataset, we are able to correctly classify with a 92.11 percent accuracy."
      ],
      "metadata": {
        "id": "Hf1l73eCghDY"
      }
    }
  ]
}