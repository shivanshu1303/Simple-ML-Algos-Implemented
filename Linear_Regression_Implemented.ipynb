{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Here, I will implement the linear regression algorithm all by myself**"
      ],
      "metadata": {
        "id": "2a6SX-vcWPvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First, we import everything"
      ],
      "metadata": {
        "id": "P_AiuRmqmPnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iGCYgdaEmR3c"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Then, we define our 'line' that we want to fit to our data"
      ],
      "metadata": {
        "id": "QCIeTIQMmYGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X,w,b):\n",
        "  return np.dot(X,w) + b"
      ],
      "metadata": {
        "id": "qxQb_OOYmf7N"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, I define our minimum squared error cost function"
      ],
      "metadata": {
        "id": "nFfYqR5hmlIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X,y,w,b):\n",
        "  m=X.shape[0]\n",
        "  predictions=model(X,w,b)\n",
        "  cost=(1/(2*m))*np.sum((predictions-y)**2)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "JBeXCozZmo96"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, I define the gradient descent algorithm that we will use to actually get to the mimimum of the error curve"
      ],
      "metadata": {
        "id": "e1EcZOUxmtHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X,y,w,b,learning_rate):\n",
        "  m=X.shape[0]\n",
        "  predictions=model(X,w,b)\n",
        "\n",
        "  w_gradient = (1/m)*np.dot(X,(predictions-y))\n",
        "  b_gradient = (1/m)*np.sum(predictions-y)\n",
        "  w-=(learning_rate)*(w_gradient)\n",
        "  b-=(learning_rate)*(b_gradient)\n",
        "\n",
        "  return w,b"
      ],
      "metadata": {
        "id": "OzxjYetRm1bJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After this comes the function that will train our model on the data we wish to train it on"
      ],
      "metadata": {
        "id": "4NDpXHmam7Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X,y,w,b,learning_rate,num_iterations):\n",
        "  costs=[]\n",
        "  for i in range(num_iterations):\n",
        "    w,b=gradient_descent(X,y,w,b,learning_rate)\n",
        "    cost=cost_function(X,y,w,b)\n",
        "    costs.append(cost)\n",
        "\n",
        "    if(i%100 == 0):\n",
        "      print(f\"At iteration number {i}, the cost is {cost}, 'w' is {w} and 'b' is {b}\")\n",
        "\n",
        "  return w,b,costs"
      ],
      "metadata": {
        "id": "6gABqTJknBRO"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After this, we enter the data that we will train our model on. Here, ive chosen a simple line of\n",
        "# y = 2*x\n",
        "## for understanding"
      ],
      "metadata": {
        "id": "Zpz7H81ynIuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([1,2,3,4,5,6,7,8,9])\n",
        "w=np.random.rand()\n",
        "y=np.array([2,4,6,8,10,12,14,16,18])\n",
        "b=0\n",
        "alpha=0.01 #Learning rate\n",
        "iterations=5000\n",
        "costs=[]"
      ],
      "metadata": {
        "id": "BEaf7y9znTv7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w,b,costs=train(X,y,w,b,alpha,iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3T0igGekPv0",
        "outputId": "cebd2c2b-b56e-4b2d-8804-0b2e42f2ceab"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At iteration number 0, the cost is 7.8787577688124495, 'w' is 1.2863561648604787 and 'b' is 0.052217841595574736\n",
            "At iteration number 100, the cost is 0.0018214240787959616, 'w' is 1.9790961602187234 and 'b' is 0.13153236927906195\n",
            "At iteration number 200, the cost is 0.0012073775660815587, 'w' is 1.982980684769435 and 'b' is 0.10708993559109235\n",
            "At iteration number 300, the cost is 0.0008003411199223012, 'w' is 1.9861433548119334 and 'b' is 0.0871895972661529\n",
            "At iteration number 400, the cost is 0.0005305265943588205, 'w' is 1.9887183113264661 and 'b' is 0.07098730454429629\n",
            "At iteration number 500, the cost is 0.00035167313076366465, 'w' is 1.9908147681059083 and 'b' is 0.05779585597903532\n",
            "At iteration number 600, the cost is 0.0002331155350479338, 'w' is 1.992521643932068 and 'b' is 0.04705575158534152\n",
            "At iteration number 700, the cost is 0.00015452659849980353, 'w' is 1.9939113339626464 and 'b' is 0.038311462296960655\n",
            "At iteration number 800, the cost is 0.00010243191059321176, 'w' is 1.9950427802878508 and 'b' is 0.031192109229611496\n",
            "At iteration number 900, the cost is 6.789961346226748e-05, 'w' is 1.9959639718907618 and 'b' is 0.02539573328343569\n",
            "At iteration number 1000, the cost is 4.5008996528772736e-05, 'w' is 1.9967139800443712 and 'b' is 0.020676487898136118\n",
            "At iteration number 1100, the cost is 2.9835365258050203e-05, 'w' is 1.9973246154742887 and 'b' is 0.016834211756374732\n",
            "At iteration number 1200, the cost is 1.9777135433628194e-05, 'w' is 1.9978217775737623 and 'b' is 0.013705939173741986\n",
            "At iteration number 1300, the cost is 1.3109780375640321e-05, 'w' is 1.9982265528963903 and 'b' is 0.011158988098339623\n",
            "At iteration number 1400, the cost is 8.690153438766951e-06, 'w' is 1.9985561095178266 and 'b' is 0.00908533255549879\n",
            "At iteration number 1500, the cost is 5.760490612767656e-06, 'w' is 1.9988244251997886 and 'b' is 0.007397020851405462\n",
            "At iteration number 1600, the cost is 3.818488630104056e-06, 'w' is 1.9990428802406042 and 'b' is 0.0060224452040571405\n",
            "At iteration number 1700, the cost is 2.531182914509488e-06, 'w' is 1.9992207401573587 and 'b' is 0.0049033045822739416\n",
            "At iteration number 1800, the cost is 1.6778593750933062e-06, 'w' is 1.9993655486720523 and 'b' is 0.003992131935107075\n",
            "At iteration number 1900, the cost is 1.112212028001223e-06, 'w' is 1.999483447669817 and 'b' is 0.0032502809319488036\n",
            "At iteration number 2000, the cost is 7.372582074469976e-07, 'w' is 1.999579437699846 and 'b' is 0.0026462868232598634\n",
            "At iteration number 2100, the cost is 4.887104713526137e-07, 'w' is 1.9996575900678868 and 'b' is 0.002154531899727229\n",
            "At iteration number 2200, the cost is 3.2395424343489787e-07, 'w' is 1.9997212195159506 and 'b' is 0.0017541589468463164\n",
            "At iteration number 2300, the cost is 2.1474136117630177e-07, 'w' is 1.9997730248132493 and 'b' is 0.0014281866103679106\n",
            "At iteration number 2400, the cost is 1.423468071013942e-07, 'w' is 1.999815203221358 and 'b' is 0.0011627891518617443\n",
            "At iteration number 2500, the cost is 9.435822414909336e-08, 'w' is 1.999849543688518 and 'b' is 0.0009467100460625727\n",
            "At iteration number 2600, the cost is 6.254776377400404e-08, 'w' is 1.9998775027257991 and 'b' is 0.0007707845484115575\n",
            "At iteration number 2700, the cost is 4.146138599376216e-08, 'w' is 1.999900266183327 and 'b' is 0.0006275509830502822\n",
            "At iteration number 2800, the cost is 2.7483740821480514e-08, 'w' is 1.9999187995467402 and 'b' is 0.0005109342644958208\n",
            "At iteration number 2900, the cost is 1.82183009910372e-08, 'w' is 1.9999338888871445 and 'b' is 0.00041598822993948586\n",
            "At iteration number 3000, the cost is 1.2076467070335485e-08, 'w' is 1.9999461742014049 and 'b' is 0.00033868585349014496\n",
            "At iteration number 3100, the cost is 8.00519526889965e-09, 'w' is 1.9999561765568714 and 'b' is 0.0002757484445438642\n",
            "At iteration number 3200, the cost is 5.3064485598324465e-09, 'w' is 1.9999643201918451 and 'b' is 0.00022450658592554565\n",
            "At iteration number 3300, the cost is 3.5175152350699597e-09, 'w' is 1.9999709505091547 and 'b' is 0.0001827869136572543\n",
            "At iteration number 3400, the cost is 2.3316749968468595e-09, 'w' is 1.999976348726016 and 'b' is 0.00014881993624654233\n",
            "At iteration number 3500, the cost is 1.5456104458873973e-09, 'w' is 1.9999807438015338 and 'b' is 0.00012116498375772108\n",
            "At iteration number 3600, the cost is 1.0245474406514384e-09, 'w' is 1.9999843221477362 and 'b' is 9.864910346877662e-05\n",
            "At iteration number 3700, the cost is 6.791474921352517e-10, 'w' is 1.9999872355360255 and 'b' is 8.03173104422393e-05\n",
            "At iteration number 3800, the cost is 4.5019029648399293e-10, 'w' is 1.9999896075343857 and 'b' is 6.539208294712723e-05\n",
            "At iteration number 3900, the cost is 2.984201596856187e-10, 'w' is 1.9999915387483753 and 'b' is 5.324038477646891e-05\n",
            "At iteration number 4000, the cost is 1.9781544027473175e-10, 'w' is 1.9999931110881948 and 'b' is 4.334681575197935e-05\n",
            "At iteration number 4100, the cost is 1.3112702724880108e-10, 'w' is 1.9999943912428129 and 'b' is 3.529175162278615e-05\n",
            "At iteration number 4200, the cost is 8.692090592394074e-11, 'w' is 1.9999954335085028 and 'b' is 2.8733546190133056e-05\n",
            "At iteration number 4300, the cost is 5.761774704507081e-11, 'w' is 1.9999962820917545 and 'b' is 2.339404078007256e-05\n",
            "At iteration number 4400, the cost is 3.8193398234697916e-11, 'w' is 1.9999969729842417 and 'b' is 1.9046766465791548e-05\n",
            "At iteration number 4500, the cost is 2.531747149689043e-11, 'w' is 1.9999975354893673 and 'b' is 1.550733865146481e-05\n",
            "At iteration number 4600, the cost is 1.6782333928436533e-11, 'w' is 1.9999979934651344 and 'b' is 1.2625636613219726e-05\n",
            "At iteration number 4700, the cost is 1.1124599552481023e-11, 'w' is 1.9999983663360534 and 'b' is 1.0279436302436477e-05\n",
            "At iteration number 4800, the cost is 7.374225524913287e-12, 'w' is 1.9999986699170116 and 'b' is 8.369226355165522e-06\n",
            "At iteration number 4900, the cost is 4.888194115829867e-12, 'w' is 1.9999989170840429 and 'b' is 6.813987433118191e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test=[200,500,1303]\n",
        "preds=model(X_test,w,b)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDTZ8TRSlHE6",
        "outputId": "184ba75b-d51a-4ba8-dce1-aa806b56700d"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 399.99982886,  999.99956381, 2605.99885437])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important learnings\n",
        "\n",
        "* Since we only have 1 feature, keep `w` as a scalar and not as a vector\n",
        "* If you want to predict for an array of `X_test`, keep `model` as `np.dot(X,w)` and not `X*w` since that wont work if `X` is anything but a scalar"
      ],
      "metadata": {
        "id": "RsPlF50ArnLI"
      }
    }
  ]
}